<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://pudongli.github.io</id>
    <title>strider</title>
    <updated>2020-09-19T05:15:08.547Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://pudongli.github.io"/>
    <link rel="self" href="https://pudongli.github.io/atom.xml"/>
    <subtitle>温故而知新</subtitle>
    <logo>https://pudongli.github.io/images/avatar.png</logo>
    <icon>https://pudongli.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, strider</rights>
    <entry>
        <title type="html"><![CDATA[mysqld-exporter一对多监控及个性化指标采集]]></title>
        <id>https://pudongli.github.io/969qj8qP3/</id>
        <link href="https://pudongli.github.io/969qj8qP3/">
        </link>
        <updated>2020-09-19T02:50:53.000Z</updated>
        <content type="html"><![CDATA[<p>mysqld-exporter是prometheus官方提供的用于监控mysql运行状态的exporter。其相关信息可以参考：https://github.com/prometheus/mysqld_exporter。<br>
项目上需要对mysql容器进行监控，由于前期没有把mysqld-exporter以sidecar的方式加入到mysql的pod中，只能以外挂的方式进行部署。部署方式如下：<br>
<img src="https://pudongli.github.io/post-images/1600484490385.png" alt="" loading="lazy"></p>
<h3 id="连接信息">连接信息</h3>
<p>mysqld-exporter与mysql实例的连接配合默认有两种，第一种是通过环境变量的方式进行配置：</p>
<pre><code>docker run -d  --restart=always  --name mysqld-exporter -p 9104:9104   -e DATA_SOURCE_NAME=&quot;user:passwd@(ip:3306)/&quot;   prom/mysqld-exporter
</code></pre>
<p>第二种方式是通过配置文件读取。当没有设置DATA_SOURCE_NAME这个环境变量时，程序会读取$HOME/.my.cnf文件，从该文件中读取相关信息，配置文件样式如下：</p>
<pre><code>[client]
user=root
host=ip
port=3306
password=123456
</code></pre>
<p>相关代码如下：<br>
mysqld_exporter/mysqld_exporter.go</p>
<pre><code>	dsn = os.Getenv(&quot;DATA_SOURCE_NAME&quot;)
        if len(dsn) == 0 {
	    var err error
	    if dsn, err = parseMycnf(*configMycnf); err != nil {
		    level.Info(logger).Log(&quot;msg&quot;, &quot;Error parsing my.cnf&quot;, &quot;file&quot;, *configMycnf, &quot;err&quot;, err)
		    os.Exit(1)
	    }
    }

...
	configMycnf = kingpin.Flag(
	&quot;config.my-cnf&quot;,
	&quot;Path to .my.cnf file to read MySQL credentials from.&quot;,).Default(path.Join(os.Getenv(&quot;HOME&quot;), &quot;.my.cnf&quot;)).String()
</code></pre>
<p>以上两种连接方式均不支持采集多个mysql实例。</p>
<h3 id="实现一对多">实现一对多</h3>
<p>参考 https://github.com/prometheus/mysqld_exporter/issues/452和https://www.cnblogs.com/wangzhao765/p/11247830.html<br>
实现思路是在prometheus访问数据接口时传一个instance参数，该参数为监控目标数据库的地址和端口（此处暂不考虑性能问题）。<br>
首先在mysqld-exporter中添加从请求中获取数据库连接地址的功能：<br>
mysqld_exporter/mysqld_exporter.go</p>
<pre><code>	target := r.URL.Query().Get(&quot;target&quot;)
	if target != &quot;&quot; {
		dsn = target
	}

	registry := prometheus.NewRegistry()
	registry.MustRegister(collector.New(ctx, dsn, metrics, filteredScrapers, logger))
</code></pre>
<p>修改prometheus配置文件：</p>
<pre><code>scrape_configs:
- job_name: 'mysqld_exporter'
static_configs:
  - targets:
    - user:password@tcp(my-mysql-network-1:3306)/
    - user:password@tcp(my-mysql-network-2:3306)/
relabel_configs:
  - source_labels: [__address__]
    target_label: __param_target
  - source_labels: [__param_target]
    target_label: instance
  - target_label: __address__
    replacement: 127.0.0.1:9115  # The mysqld_exporter's real hostname:port.
</code></pre>
<p>加载prometheus配置文件，重新编译mysqld-exporter并运行，从日志中可以看到prometheus请求查询时带来的数据库连接信息。<br>
<img src="https://pudongli.github.io/post-images/1600486718105.png" alt="" loading="lazy"></p>
<h3 id="指标采集">指标采集</h3>
<p>想通过gitd计算mysql从库落后主库事件数,但是默认采集到的指标里并没有Retrieved_Gtid_Set和Executed_Gtid_Set这两个指标。查找相关代码：<br>
mysqld_exporter/collector/slave_status.go</p>
<pre><code>		for i, col := range slaveCols {
            	if value, ok := parseStatus(*scanArgs[i].(*sql.RawBytes)); ok {
                    ...
                }
            }                   
</code></pre>
<p>mysqld_exporter/collector/collector.go</p>
<pre><code>func parseStatus(data sql.RawBytes) (float64, bool) {
    if bytes.Equal(data, []byte(&quot;Yes&quot;)) || bytes.Equal(data, []byte(&quot;ON&quot;)) {
	return 1, true
    }
    if bytes.Equal(data, []byte(&quot;No&quot;)) || bytes.Equal(data, []byte(&quot;OFF&quot;)) {
	return 0, true
    }
	// SHOW SLAVE STATUS Slave_IO_Running can return &quot;Connecting&quot; which is a non-running state.
    if bytes.Equal(data, []byte(&quot;Connecting&quot;)) {
	return 0, true
    }
    // SHOW GLOBAL STATUS like 'wsrep_cluster_status' can return &quot;Primary&quot; or &quot;non-Primary&quot;/&quot;Disconnected&quot;
    if bytes.Equal(data, []byte(&quot;Primary&quot;)) {
	return 1, true
    }
    if strings.EqualFold(string(data), &quot;non-Primary&quot;) || bytes.Equal(data, []byte(&quot;Disconnected&quot;)) {
	return 0, true
    }
    if logNum := logRE.Find(data); logNum != nil {
	value, err := strconv.ParseFloat(string(logNum), 64)
	return value, err == nil
    }
    value, err := strconv.ParseFloat(string(data), 64)
    return value, err == nil
}
</code></pre>
<p>从上面的代码可以看到在处理mysql指标时会先对其value进行验证，parseStatus函数会对特定格式的value进行值转换并提供给后续处理。Retrieved_Gtid_Set和Executed_Gtid_Set这两个指标的值的格式稍微复杂，在进行parseStatus时候校验不通过返回false后就被丢弃了。要想采集这些value值复杂的指标，在parseStatus之前单独处理即可。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[容器网络]]></title>
        <id>https://pudongli.github.io/i4Ij7brxY/</id>
        <link href="https://pudongli.github.io/i4Ij7brxY/">
        </link>
        <updated>2020-09-03T05:56:22.000Z</updated>
        <content type="html"><![CDATA[<h2 id="几种模式">几种模式</h2>
<h3 id="默认bridge">默认bridge</h3>
<ul>
<li>
<p>docker0虚拟网桥</p>
<p>docker安装时会创建一个名为docker0的linux bridge。如果不指定--network，创建的容器默认都会挂到docker0上。虚拟网桥的工作方式和物理交换机类似，这样主机上的所有容器就通过交换机连在一个二层网络中。<br>
从docker0子网中分配一个IP给容器使用，并设置docker0的IP地址为容器的默认网关。在主机上创建一对虚拟网卡veth pair设备，Docker将veth pair设备的一端放在新创建的容器中，并命名为eth0（容器的网卡），另一端放在主机中，以vethxxx这样类似的名字命名，并将这个网络设备加入到docker0网桥中。可以通过brctl show命令查看。<br>
bridge模式是docker的默认网络模式，不写--net参数，就是bridge模式。使用docker run -p时，docker实际是在iptables做了DNAT规则，实现端口转发功能。可以使用iptables -t nat -vnL查看。</p>
</li>
</ul>
<h3 id="host模式">host模式</h3>
<ul>
<li>
<p>和宿主机公用network namespace</p>
<p>容器不会虚拟出自己的网卡和ip，而是使用宿主机的IP和端口。</p>
</li>
</ul>
<h3 id="none">none</h3>
<h3 id="container模式">container模式</h3>
<ul>
<li>容器启动的时候选择和已存在的容器共用同一个network namespaces</li>
</ul>
<h3 id="-link">--link</h3>
<h2 id="使用方式">使用方式</h2>
<h3 id="docker-run-netxxx">docker run --net=xxx</h3>
<h2 id="访问场景">访问场景</h2>
<h3 id="主机内互相访问">主机内互相访问</h3>
<figure data-type="image" tabindex="1"><img src="https://pudongli.github.io/post-images/1599112774527.png" alt="" loading="lazy"></figure>
<h3 id="主机内的container访问外部">主机内的container访问外部</h3>
<figure data-type="image" tabindex="2"><img src="https://pudongli.github.io/post-images/1599112779792.png" alt="" loading="lazy"></figure>
<h3 id="跨主机网络">跨主机网络</h3>
<ul>
<li>
<p>flannel</p>
<ul>
<li>
<p>实现模式</p>
<ul>
<li>
<p>VXLAN</p>
<ul>
<li>VTEP<br>
<img src="https://pudongli.github.io/post-images/1599112887599.png" alt="" loading="lazy"><br>
设计思想：在现有的三层网络中之上，覆盖一层虚拟的、由内核VXLAN模块负责维护的二层网络，使得连接在这个VXLAN二层网络上的主机之间可以像在同一个局域网里自由通信。<br>
为了能够在二层网络上打通隧道，VXLAN会在宿主机上设置一个特殊的网络设备作为隧道的两端。这个设备就是VTEP。</li>
</ul>
</li>
<li>
<p>host-gw</p>
</li>
<li>
<p>udp<br>
<img src="https://pudongli.github.io/post-images/1599112870279.png" alt="" loading="lazy"><br>
最简单，最早支持，性能最差，已被弃用。<br>
udp模式提供一个三层overlay网络，首先对发出端的IP包进行UDP封装，然后在接收端进行解封拿到原始ip，进而转发给目标容器。缺点是需要在用户态和内核态之间进行数据拷贝，需要切换上下文。</p>
</li>
</ul>
</li>
<li>
<p>子网</p>
<p>在由flannel管理的容器网络里，一个宿主机上的所有容器都属于该宿主机被分配的一个子网。子网与宿主机的对应关系保存在etcd内。</p>
</li>
</ul>
</li>
</ul>
<h2 id="k8s网络模型与cni插件">k8s网络模型与CNI插件</h2>
<p>网络插件要做的事，是通过某种方式把不同宿主机上的特殊设备联通，从而达到容器跨主机通信的目的。<br>
k8s通过CNI接口维护了一个单独的网桥来代替docker0，这个网桥的名字叫做CNI网桥。在宿主机上的设备名称默认为cni0。</p>
<h3 id="设计思想">设计思想</h3>
<p>k8s在启动infra容器之后，就可以直接调用CNI网络插件，为这个infra容器的networknamespace，配置符合预期的网络栈。</p>
<h3 id="执行文件">执行文件</h3>
<p>[root@master1 bin]# ll<br>
total 86000<br>
-rwxr-xr-x 1 root root 4559393 Aug 19 17:53 bandwidth<br>
-rwxr-xr-x 1 root root 32769056 Aug 19 17:53 calico<br>
-rwxr-xr-x 1 root root 31970176 Aug 19 17:53 calico-ipam<br>
-rwxr-xr-x 1 root root 3069034 Aug 19 17:53 flannel<br>
-rwxr-xr-x 1 root root 3957620 Aug 19 17:53 host-local<br>
-rwxr-xr-x 1 root root 3650379 Aug 19 17:53 loopback<br>
-rwxr-xr-x 1 root root 4327403 Aug 19 17:53 portmap<br>
-rwxr-xr-x 1 root root 3736919 Aug 19 17:53 tuning<br>
[root@master1 bin]# pwd<br>
/opt/cni/bin</p>
<ul>
<li>main插件，用来创建具体网络设备</li>
<li>IPAM插件，负责分配IP地址</li>
<li>内置CNI插件，比如flannel</li>
</ul>
<h3 id="网络方案">网络方案</h3>
<ul>
<li>首先实现网络方案本身</li>
<li>然后实现该网络方案对应的CNI插件</li>
</ul>
<h3 id="三层网络方案">三层网络方案</h3>
<ul>
<li>
<p>flannel host-gw</p>
<p>工作原理：将每个Flannel子网的下一跳设置成该子网对应的宿主机的IP地址。也就是说这台主机会充当这条容器通信路径里的网关。<br>
子网和主机的信息都保存在etcd里，flanneld只需要watch这些数据的变化然后实时更新路由表即可。<br>
在这种模式下容器通信的适合就免除了额外的封包和解包带来的性能损耗。</p>
</li>
<li>
<p>calico</p>
<ul>
<li>
<p>BGP</p>
<p>Border Gateway Protocol，边界网关协议。<br>
大规模网络中实现节点路由信息共享的一种协议</p>
</li>
<li>
<p>架构</p>
<ul>
<li>
<p>CNI插件</p>
</li>
<li>
<p>Felix</p>
<p>是一个daemonSet，负责在宿主机上插入路由规则，以及维护calico所需的网络设备等工作。</p>
</li>
<li>
<p>BIRD</p>
<p>BGP客户端，负责在集群内分发路由规则信息。</p>
</li>
</ul>
</li>
<li>
<p>和flannel host-gw的区别</p>
<ul>
<li>Flannel通过ETCD和宿主机上的flanneld来维护路由信息，而calico使用BGP来自动在整个集群中分发路由信息。</li>
<li>calico不会在宿主机上创建网桥设备。</li>
</ul>
</li>
<li>
<p>工作方式<br>
<img src="https://pudongli.github.io/post-images/1599112930444.png" alt="" loading="lazy"></p>
<ul>
<li>默认node-to-node mesh<br>
route reflector</li>
<li>IPIP<br>
<img src="https://pudongli.github.io/post-images/1599112970525.jpg" alt="" loading="lazy">
<ul>
<li>访问路径<br>
<img src="https://pudongli.github.io/post-images/1599112944864.png" alt="" loading="lazy"></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[golang垃圾收集器]]></title>
        <id>https://pudongli.github.io/erKeo3HJm/</id>
        <link href="https://pudongli.github.io/erKeo3HJm/">
        </link>
        <updated>2020-08-12T06:46:42.000Z</updated>
        <content type="html"><![CDATA[<p>本文是读draveness大佬的文章后的摘要，原文地址 <a href="https://draveness.me/golang/docs/part3-runtime/ch07-memory/golang-garbage-collector/">https://draveness.me/golang/docs/part3-runtime/ch07-memory/golang-garbage-collector/</a></p>
<h1 id="go垃圾收集器">GO垃圾收集器</h1>
<h2 id="设计原理">设计原理</h2>
<h3 id="内存管理的组件">内存管理的组件</h3>
<pre><code>用户程序Mutator通过内存分配器Allocator在堆上申请内存，而垃圾收集器Collector负责回收堆上的内存空间，内存分配器和垃圾收集器共同管理程序中的堆内存空间
</code></pre>
<h4 id="标记清除">标记清除</h4>
<pre><code>标记清除Mark-Sweep算法是最常见的垃圾收集算法。标记清除收集器是跟踪式垃圾收集器，其执行过程可分成Mark和Sweep两个阶段：
	1、标记阶段：从根对象出发查找并标记堆中所有存活的对象；
	2、清除阶段：遍历堆中所有的对象，回收未被标记的垃圾对象并将回收的内存加入到空闲链表中
</code></pre>
<p><img src="https://pudongli.github.io/post-images/1597215066449.png" alt="" loading="lazy"><br>
<strong>标记清除的标记阶段</strong><br>
<img src="https://pudongli.github.io/post-images/1597215073028.png" alt="" loading="lazy"><br>
<strong>标记清除的清除阶段</strong></p>
<pre><code>使用标记清除算法，垃圾收集器需要从垃圾收集的根节点出发，递归遍历这个对象指向的子节点并将所有可达的对象标记为存活；标记结束后，垃圾收集器会依次遍历堆中的对象并清除其中的垃圾。
缺点：
* 需要STW
* 标记需要扫描整个heap
* 清楚数据会产生heap碎片
</code></pre>
<h4 id="三色抽象">三色抽象</h4>
<pre><code>为了解决标记清除算法带来的长时间STW，多数现代追踪式垃圾收集器都会实现三色标记算法的变种以缩短STW的时间。三色标记算法将程序中的对象分为白色、灰色、黑色三类
* 白色对象-潜在的垃圾，其内存可能会被垃圾收集器回收；
* 黑色对象-活跃的对象，包括不存在任何外部引用指针的对象已经从根可达的对象；
* 灰色对象-活跃的对象，因为存在指向白色对象的外部指针，垃圾收集器会扫描这些对象的子对象；
</code></pre>
<h4 id="屏障技术">屏障技术</h4>
<pre><code>  * 强三色不变性-黑色对象不会指向白色对象，只会指向灰色对象或者黑色对象；
  * 弱三色不变性-黑色对象指向的白色对象必须包含一条从灰色对象经由多个白色对象的可达路径；
</code></pre>
<h5 id="插入写屏障">插入写屏障</h5>
<pre><code>在A对象引用B对象的时候，B对象被标记为灰色。满足强三色不变性
</code></pre>
<pre><code>添加下游对象(当前下游对象slot, 新下游对象ptr) {
    //1
    标记灰色(新下游对象ptr)
    //2 
    当前下游对象slot = 新下游对象ptr
}
</code></pre>
<pre><code>插入写屏障机制在栈空间的对象操作中不使用，而仅仅作用于堆空间对象的操作中。
在堆空间扫描结束以后要对栈重新进行三色标记扫描。需要STW，直到栈空间标记结束。
</code></pre>
<h5 id="删除写屏障">删除写屏障</h5>
<pre><code>被删除的对象如果自身是白色或者灰色，那么被标记为灰色。满足弱三色不变性
</code></pre>
<pre><code>添加下游对象(当前下游对象slot， 新下游对象ptr) {
    //1
    if (当前下游对象slot是灰色 || 当前下游对象slot是白色) {
        标记灰色(当前下游对象slot) //slot为被删除对象， 标记为灰色
    }
    //2 
    当前下游对象slot = 新下游对象ptr
}
</code></pre>
<pre><code>删除写屏障回收精度低，一个对象即使被删除了最后一个指向它的指针，也仍然可以活过这一轮，在下一轮gc中被清理掉。
</code></pre>
<h5 id="混合写屏障">混合写屏障</h5>
<pre><code>实现步骤
1、gc开始时将栈上的对象全部扫描并标记为黑色，（之后不再进行第二次扫描，无需STW）
2、GC期间，任何堆上创建的新对象均标记为黑色；
3、被删除的对象标记为灰色；
4、被添加的对象标记为灰色；
</code></pre>
<pre><code>
添加下游对象(当前下游对象slot, 新下游对象ptr) {
    //1
    标记灰色(当前下游对象slot) //只要当前下游对象被移走，就标记灰色
    //2
    标记灰色(新下游对象ptr)
    //3 
    当前下游对象slot = 新下游对象ptr
}
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[golang内置的print/println函数和fmt、log标准库包中的打印函数有什么区别]]></title>
        <id>https://pudongli.github.io/HOiLnuUYA/</id>
        <link href="https://pudongli.github.io/HOiLnuUYA/">
        </link>
        <updated>2020-08-12T06:45:28.000Z</updated>
        <content type="html"><![CDATA[<p>1.内置的print/println函数总是写入到标准错误(os.Stderr)。fmt标准包里的打印函数总是写入到标准输出(os.Stdout)。log里的打印函数默认写入标准错误，可以通过log.SetOutput函数来设置。<br>
2.print/println函数的调用不能接受数组和结构体参数。<br>
3.对于组合类型的参数，内置的print/println函数将输出参数的底层值部的地址，而fmt和log将输出参数的字面值。<br>
4.在Go1.14版本对于标准编译器，调用print/println函数不会使调用参数引用的指针的值逃逸到堆上，而fmt和log标准库中打印函数将使调用参数引用的值逃逸到堆上。<br>
5.如果一个实参有String() string或Error() string方法，那么fmt和log标准库包里的打印函数在打印参数时会调用这两个方法，而内置的print/println函数则会忽略参数的这些方法。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[golang中slice扩容]]></title>
        <id>https://pudongli.github.io/-lmXlat1b/</id>
        <link href="https://pudongli.github.io/-lmXlat1b/">
        </link>
        <updated>2020-08-12T06:21:40.000Z</updated>
        <content type="html"><![CDATA[<p>slice扩容是在slice使用过程中经常使用到的场景，扩容发生在append的时候，当slice的cap不足以容纳新的元素时，就会进行growSlice。当前场景下go版本为1.14.2<br>
slice的扩容分为常规场景和非常规场景，先看一下常规场景下：</p>
<pre><code>slice1 := make([]int,1,)
fmt.Println(&quot;cap of slice1&quot;,cap(slice1))
slice1 = append(slice1,1)
fmt.Println(&quot;cap of slice1&quot;,cap(slice1))
slice1 = append(slice1,2)
fmt.Println(&quot;cap of slice1&quot;,cap(slice1))

fmt.Println()

slice1024 := make([]int,1024)
fmt.Println(&quot;cap of slice1024&quot;,cap(slice1024))
slice1024 = append(slice1024,1)
fmt.Println(&quot;cap of slice1024&quot;,cap(slice1024))
slice1024 = append(slice1024,2)
fmt.Println(&quot;cap of slice1024&quot;,cap(slice1024))


输出：
cap of slice1 1
cap of slice1 2
cap of slice1 4

cap of slice1024 1024
cap of slice1024 1280
cap of slice1024 1280
</code></pre>
<p>然后看一下非常规场景：</p>
<pre><code>a := []byte{1, 0}
a = append(a, 1)
fmt.Println(&quot;cap of a is &quot;, cap(a))

b := []int{1, 1}
b = append(b, 4, 5, 6)
fmt.Println(&quot;cap of b is &quot;, cap(b))

c := []int32{1, 1}
c = append(c, 2, 5, 6)
fmt.Println(&quot;cap of c is &quot;, cap(c))

输出
8
6
8
</code></pre>
<p>下面说一下slice扩容的步骤，并解答非常规场景下的输出。<br>
slice的扩容主要分为3步<br>
1、计算预计大小<br>
如果预估扩容后的slice的容量大于oldCap的2倍，则预估容量为扩容后的大小<br>
如果预估扩容后的slice的容量小于oldCap的2倍<br>
a:如果oldCap小于1024，则预估容量为oldCap<em>2<br>
b:如果oldCap大于1024，则预估容量为oldCap</em>1.25<br>
2、预估内存大小<br>
计算扩容后需要占用的内存大小：在64位机器下byte为1字节，int为8字节，int32为4字节，非常规场景下的b的预估内存为5*8=40<br>
3、计算最终容量大小<br>
进行内存对齐<br>
<code>capmem = roundupsize(uintptr(newcap) * sys.PtrSize)</code> roundupsize向上取整<br>
<a href="https://golang.org/src/runtime/sizeclasses.go">https://golang.org/src/runtime/sizeclasses.go</a><br>
<img src="https://pudongli.github.io/post-images/1597213503857.png" alt="" loading="lazy"><br>
sizeclass中定义了对象大小，最小8b，最大为32K<br>
b的预估大小为40，这里没有40的类型，于是选择大于40字节，并最接近大小的48字节。48/8=6，所以b扩容后的cap为6</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[记录一次k8s下部署的tomcat服务报错]]></title>
        <id>https://pudongli.github.io/w0dyewgIS/</id>
        <link href="https://pudongli.github.io/w0dyewgIS/">
        </link>
        <updated>2020-08-12T06:08:06.000Z</updated>
        <content type="html"><![CDATA[<h4 id="在k8s集群内部署了一个tomcat服务配置nodeport供外部访问-页面报错找不到configjs">在k8s集群内部署了一个tomcat服务，配置nodeport供外部访问。页面报错找不到config.js</h4>
<figure data-type="image" tabindex="1"><img src="https://pudongli.github.io/post-images/1597212531069.png" alt="" loading="lazy"></figure>
<h4 id="进入tomcat容器后在rootconfig目录下可以找到config这个文件权限是777-进一步ls-la发现configjs是一个软链接指向的是dataconfigjsdata是一个带有时间戳的目录的软链接这个目录下存放的是真实文件">进入tomcat容器后，在ROOT/config目录下可以找到config这个文件，权限是777。进一步ls -la发现config.js是一个软链接，指向的是..data/config.js，..data是一个带有时间戳的目录的软链接，这个目录下存放的是真实文件。</h4>
<h4 id="为什么tomcat读不到configjs呢原因是tomcat需要在配置文件中配置allowlinkingtrue才可以使用软链接默认是不支持的">为什么tomcat读不到config.js呢，原因是tomcat需要在配置文件中配置allowLinking=true才可以使用软链接，默认是不支持的。</h4>
<h4 id="当容器第一次启动前kubelet先将configmap中的内容下载到pod对应的volume目录下比如varlibkubeletpodspod-uidvolumeskubernetesio~configmapextra-cfg-为了保证对此volume下内容的更新是原子操作更新目录时所以会通过软链接的方式进行更新">当容器第一次启动前，kubelet先将configmap中的内容下载到Pod对应的volume目录下，比如/var/lib/kubelet/pods/{POD UID}/volumes/kubernetes.io~configmap/extra-cfg。为了保证对此volume下内容的更新是原子操作（更新目录时），所以会通过软链接的方式进行更新。</h4>
<p>参考：<a href="https://blog.fatedier.com/2020/04/17/pod-loopcrash-of-k8s-subpath/">https://blog.fatedier.com/2020/04/17/pod-loopcrash-of-k8s-subpath/</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[关于client-go报错]]></title>
        <id>https://pudongli.github.io/72UPyoiW/</id>
        <link href="https://pudongli.github.io/72UPyoiW/">
        </link>
        <updated>2020-08-12T05:35:31.000Z</updated>
        <content type="html"><![CDATA[<h3 id="go-get-client-go时报错">go get client-go时报错</h3>
<pre><code>k8s.io/client-go/rest

../../../../goworkspace/pkg/mod/k8s.io/client-go@v11.0.0+incompatible/rest/request.go:598:31: not enough arguments in call to watch.NewStreamWatcher

have (*versioned.Decoder)

want (watch.Decoder, watch.Reporter)
</code></pre>
<h3 id="解决方法">解决方法</h3>
<p>可以尝试手动替换k8s.io/apimachinery@v0.17.0为k8s.io/apimachinery@release-1.14来解决。在终端执行# go mod download -json k8s.io/apimachinery@release-1.14最终gomod的关于k8s的所有依赖文件如下所示：</p>
<pre><code>1 require (
2 　　k8s.io/api v0.17.0 // indirect
3 　　k8s.io/apimachinery v0.17.0
4 　　k8s.io/client-go v11.0.0+incompatible
5 　　k8s.io/utils v0.0.0-20191114184206-e782cd3c129f // indirect
6 　　sigs.k8s.io/yaml v1.1.0 // indirect
7 )
8
9 replace (
10     k8s.io/api =&gt; k8s.io/api v0.0.0-20191004102349-159aefb8556b
11     k8s.io/apiextensions-apiserver =&gt; k8s.io/apiextensions-apiserver v0.0.0-20191004105649-b14e3c49469a
12     k8s.io/apimachinery =&gt; k8s.io/apimachinery v0.0.0-20191004074956-c5d2f014d689
13     sigs.k8s.io/controller-runtime =&gt; sigs.k8s.io/controller-runtime v0.3.0
14 )
</code></pre>
]]></content>
    </entry>
</feed>